{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e24d64",
   "metadata": {},
   "source": [
    "# STA410 Week 10 Programming Assignment (10 points)\n",
    "\n",
    "Welcome.\n",
    "\n",
    "## Rules\n",
    "\n",
    "\n",
    "0. **This is a paired or individual assignment.** Specific code solutions submitted for these assignments must be created either individually or in the context of a paired effort: ***group efforts of three or more are students are not allowed.*** Please seek homework partners in-person or on the course discussion board on piazza. **Paired students each separately submit their (common) work, including (agreeing) contribution of work statements for each problem.**\n",
    "  \n",
    "   > Students choosing to work individually must work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity)  of \"honesty, trust, fairness, respect, responsibility and courage.\" Students working in pairs may share work without restriction within their pair, but must otherwise work in accordance with the [University Student Academic Integrity values](https://www.artsci.utoronto.ca/current/academic-advising-and-support/student-academic-integrity) noted above. ***Getting and sharing \"hints\" from other classmates is allowed; but, the eventual code creation work and submission must be your own individual or paired creation.***\n",
    "   \n",
    "   \n",
    "1. **Do not delete or replace cells**: this erases `cell ids` upon which automated code tests are based.\n",
    "\n",
    "    - ***If you accidentally delete a required cell*** try \"Edit > Undo Delete Cells\" in the notebook editor; otherwise, redownload the notebook (so it has the correct required `cells ids`) and repopulate it with your answers (assuming you don't overwrite them when you redownload the notebook).\n",
    "\n",
    "   - ***You may add cells for scratch work*** but if required answers are not submitted through the provided cells where the answers are requested your answers may not be marked.\n",
    "\n",
    "  > You may check if `cell ids` are present and working by running the following command in a cell \n",
    "  >\n",
    "  > `! grep '\"id\":' <path/to/notebook>.ipynb`\n",
    "  >\n",
    "  > and making sure the `cell ids` **do not change** when you save your notebook.\n",
    "  >\n",
    "  >> ***If you are working in any environment other than*** [UofT JupyterLab](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw0&branch=master&urlpath=/lab/tree/sta410hw0), [UofT JupyterHub](https://jupyter.utoronto.ca/hub/user-redirect/git-pull?repo=https://github.com/pointOfive/sta410hw0&branch=master), or [Google Colab](https://colab.research.google.com/github/pointOfive/sta410hw0/blob/master/sta410hw0.ipynb), your system must meet the following versioning requirements \n",
    "   >>\n",
    "   >>   - [notebook format >=4.5](https://github.com/jupyterlab/jupyterlab/issues/9729) \n",
    "   >>   - jupyter [notebook](https://jupyter.org/install#jupyter-notebook) version [>=6.2](https://jupyter-notebook.readthedocs.io/en/stable/) for \"classic\" notebooks served by [jupyterhub](https://jupyterhub.readthedocs.io/en/stable/quickstart.html)\n",
    "   >>   - [jupyterlab](https://jupyter.org/install) version [>=3.0.13](https://github.com/jupyterlab/jupyterlab/releases/tag/v3.0.13) for \"jupyterlab\" notebooks  \n",
    "   >>    \n",
    "   >> otherwise `cell ids` will not be supported and you will not get any credit for your submitted homework.  \n",
    "      \n",
    "2. **No cells may have any runtime errors** because this causes subsequent automated code tests to fail and you will not get marks for tests which fail because of previous runtime errors. \n",
    "\n",
    "  - Run time errors include, e.g., unassigned variables, mismatched parentheses, and any code which does not work when the notebook cells are sequentially run, even if it was provided for you as part of the starter code. ***It is best to restart and re-run the cells in your notebook to ensure there are no runtime errors before submitting your work.***\n",
    "    \n",
    "  - The `try`-`except` block syntax catches runtime errors and transforms them into `exceptions` which will not cause subsequent automated code tests to fail.  \n",
    "\n",
    "\n",
    "3. **No jupyter shortcut commands** such as `! python script.py 10` or `%%timeit` may be included in the final submission as they will cause subsequent automated code tests to fail.\n",
    "\n",
    "  - ***Comment out ALL jupyter shortcut commands***, e.g., `# ! python script.py 10` or `# %%timeit` in submitted notebooks.\n",
    "\n",
    "\n",
    "4. **Python library imports are limited** to only libraries imported in the starter code and the [standard python modules](https://docs.python.org/3/py-modindex.html). Importing additional libraries will cause subsequent automated code tests to fail.\n",
    "\n",
    "  > Unless a problem instructs differently, you may use any functions available from the libraries imported in the starter code; otherwise, you are expected to create your own Python functionality based on the Python stdlib (standard libary, i.e., base Python and standard Python modules).\n",
    "\n",
    "\n",
    "5. You are welcome and encouraged to adapt code you find available online into your notebook; however, if you do so you must provide a link to the utilized resource. ***If failure to cite such references is identified and confirmed, your mark will be immediately reduced to 0.***  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06837861",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Unless a problem instructs differently, you may use any functions available from the following library imports\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import sklearn\n",
    "import statsmodels.api as sm\n",
    "from scipy.special import expit as invlogit\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72c4f22",
   "metadata": {},
   "source": [
    "# Problem 0 (required)\n",
    "\n",
    "Are you working with a partner to complete this assignment?  \n",
    "- If not, assign  the value of `None` into the variable `Partner`.\n",
    "- If so, assign the name of the person you worked with into the variable `Partner`.\n",
    "    - Format the name as `\"<First Name> <Last Name>\"` as a `str` type, e.g., \"Scott Schwartz\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e608b77",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Partner = #None\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb02ffe",
   "metadata": {},
   "source": [
    "What was your contribution in completing the code for this assignments problems? Assign one of the following into each of the `Problem_X` variables below.\n",
    "\n",
    "- `\"I worked alone\"`\n",
    "- `\"I contributed more than my partner\"`\n",
    "- `\"My partner and I contributed equally\"`\n",
    "- `\"I contributed less than my partner\"`\n",
    "- `\"I did not contribute\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5beae8e9",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Required: only worth points when not completed, in which case, you'll lose points\n",
    "Problem_1 = #\"I worked alone\"\n",
    "Problem_2 = #\"I worked alone\"\n",
    "# This cell will produce a runtime error until you assign a value to this variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e19ba05a",
   "metadata": {},
   "source": [
    "# Problem 1 (5 points)\n",
    "\n",
    "Define the function `logistic_regression_IRLS(X, y, beta0, k)` which returns $\\beta^{(k)}$ of the ***iteratively reweighted least squares*** (IRLS) fit \n",
    "\n",
    "1. $\\tilde y^{(t)} = \\underline{X\\beta^{(t)}}+ \\overset{(t)}{W}{}^{-1}(y- \\overset{(t)}{E[y]})$\n",
    "    - $\\overset{(t)}{E[y_i]}= \\frac{1}{1+\\exp(-z_i\\beta^{(t)})}$\n",
    "    - $\\overset{(t)}{W_{ij}} = 0 \\text{ for } i\\not=j \\text{ and } \\overset{(t)}{W_{ii}} = \\overset{(t)}{E[y_i]} \\left(1-\\overset{(t)}{E[y_i]} \\right)$\n",
    "\n",
    "\n",
    "2. $\\beta^{(t+1)} = \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} \\tilde y^{(t)} \\quad \\underset{\\text{solved as}}{\\overset{\\text{efficiently}}{\\Longrightarrow}} \\quad \\left(X^T \\overset{(t)}{W}X\\right)\\beta^{(t+1)} = X^T\\overset{(t)}{W} \\tilde y^{(t)}$\n",
    "\n",
    "\n",
    "of the logistic regression model\n",
    "   \n",
    "$\\quad\\quad\\quad\\displaystyle \\Pr(y_i=1) = \\frac{1}{1+\\exp(-z_i\\beta)}$\n",
    "\n",
    "However, rather than using the IRLS form exactly, instead use the standard form of ***Newton's method*** where the $\\beta^{(t+1)}$ update above is instead reformulated as \n",
    "\n",
    "\\begin{align*}\n",
    "\\beta^{(t+1)} & = {} \\underline{\\beta^{(t)}} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} (\\tilde y^{(t)}-\\underline{X\\beta^{(t)}})\\\\\n",
    "& = {} \\underline{\\beta^{(t)}} + \\underbrace{\\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T (y- \\overset{(t)}{E[y]}) }_{\\text{use } \\texttt{np.linalg.solve} \\textbf{ not } \\text{matrix inversion}}\n",
    "\\end{align*}\n",
    "\n",
    "The reasons for using the ***Newton's method*** version instead of the ***IRLS*** version of the update is that it avoids multiplying by the $\\overset{(t)}{W}$ matrix twice and to make the value of the next iteration $\\beta^{(t+1)}$ an addiive offset adjustment to the value of the previous iteration $\\beta^{(t)}$, which is often more numerically accurate than recreating the target value as a fresh computation.\n",
    "\n",
    "*This problem is inspired by Subsection 2.2.1.1 **Iteratively Reweighted Least Squares** of Section 2.2.1 **Newton's Method and Fisher Scoring** in Chapter 2.2 **Multivariate Problems** of the Givens and Hoeting **Computational Statistics** textbook (pages 34-38).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c43989",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "from scipy.special import expit as invlogit\n",
    "def logistic_regression_IRLS(X, y, beta0, k):\n",
    "    '''\n",
    "    Computes IRLS estimation of logistic regression\n",
    "    \n",
    "    X    : (np.array) n by p design matrix\n",
    "    Y    : (np.array) n binary outcomes\n",
    "    beta0: (np.array) p coefficients\n",
    "    k    : (int)      number of IRLS to beta0\n",
    "    \n",
    "    returns: (np.array) beta coefficients after k IRLS updates to beta0\n",
    "    '''\n",
    "\n",
    "    beta_t = np.zeros((t,X.shape[1]))\n",
    "    beta_t[0,:] = beta0\n",
    "    \n",
    "    for i in range(1,t):\n",
    "        pass #<complete>\n",
    "        \n",
    "    return beta_t[-1,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8919df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work\n",
    "\n",
    "# You are welcome to add as many new cells into this notebook as you would like.\n",
    "# Just do not leave in a state that will produce a runtime errors when notebook cells are run sequentially.\n",
    "\n",
    "# Any cells included for scratch work that are no longer needed may be deleted so long as \n",
    "# - all the required functions are still defined and available when called\n",
    "# - no cells requiring variable assignments are deleted.\n",
    "\n",
    "# None of this will not cause problems with `cell ids` assuming your versioning supports `cell ids`\n",
    "# (as UofT JupyterHub, UofT JupyterLab, an Google Colab will).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "606672cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell for scratch work\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b352ab",
   "metadata": {},
   "source": [
    "## Hints\n",
    "\n",
    "- This algorithm is specified in the course notes and is also discussed in Keith Knight's STA410 [notes13.pdf document](https://q.utoronto.ca/courses/296804/files?preview=25407719) and [logistic.pdf document](https://q.utoronto.ca/courses/296804/files?preview=25407763).\n",
    "- For `W.shape = (n,1)` the `X.T.np.diag(W)` computation is the same as `(X*W).T` based on `numpy` broadcasting\n",
    "- Your algorithm can be checked against\n",
    "\n",
    "    - `statsmodels`\n",
    "    \n",
    "        ```python\n",
    "        # https://www.geeksforgeeks.org/logistic-regression-using-statsmodels/\n",
    "        import statsmodels.api as sm\n",
    "        log_reg = sm.Logit(y[:,np.newaxis], X).fit()\n",
    "        log_reg.summary()\n",
    "        ```\n",
    "\n",
    "    - `scikit-learn`\n",
    "    \n",
    "        ```python\n",
    "        # https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html\n",
    "        # https://stats.stackexchange.com/questions/203740/logistic-regression-scikit-learn-vs-statsmodels\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        logreg = LogisticRegression(penalty='none', fit_intercept=False)\n",
    "        logreg.fit(X, y)\n",
    "        logreg.coef_\n",
    "        ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b37debd",
   "metadata": {},
   "source": [
    "## Problem 1 Questions 0-1 (2.5 points)\n",
    "\n",
    "Your function will be tested against\n",
    "\n",
    "```python \n",
    "from scipy.special import expit as invlogit\n",
    "np.random.seed(seed)\n",
    "X = np.random.normal(mu, sd, (n,p))\n",
    "X[:,0] = 1\n",
    "beta_mean,beta_sd = 0,1\n",
    "beta = np.random.normal(beta_mean, beta_sd, p)\n",
    "y = (np.random.uniform(size=n)<invlogit(X.dot(beta))).astype(int)\n",
    "```\n",
    "\n",
    "- You do not need to make any variable assignments: your function will be called based on the parameterization specified in the problem prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4717a6",
   "metadata": {},
   "source": [
    "## Problem 1 Questions 2-6 (2.5 points)\n",
    "\n",
    "2. (0.5 points) Which of the following most directly indicates that the updated iteration is the solution to a ***weighted least squares linear regression*** problem? \n",
    "\n",
    "    - (A) $\\tilde y^{(t)} = \\underline{X\\beta^{(t)}}+ \\overset{(t)}{W}{}^{-1}(y- \\overset{(t)}{E[y]})$\n",
    "    - (B) $\\beta^{(t+1)} = \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} \\tilde y^{(t)}$\n",
    "    - (C) $\\beta^{(t+1)} = \\underline{\\beta^{(t)}} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} (\\tilde y^{(t)}-\\underline{X\\beta^{(t)}})$\n",
    "    - (D) $\\beta^{(t+1)} = \\beta^{(t)} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T (y- \\overset{(t)}{E[y]})$\n",
    "\n",
    "\n",
    "3. (0.5 points) Which of the following is the ***Hessian*** of the ***logistic regression likelihood***?\n",
    "\n",
    "    - (A) $\\overset{(t)}{W}{}^{-1}$\n",
    "    - (B) $X^T\\overset{(t)}{W}$\n",
    "    - (C) $X^T \\overset{(t)}{W}X$\n",
    "    - (D) $\\left(X^T \\overset{(t)}{W}X\\right)^{-1}$ \n",
    "\n",
    "\n",
    "4. (0.5 points) Which of the following is the ***gradient*** of the ***logistic regression likelihood***?\n",
    "\n",
    "    - (A) $(y- \\overset{(t)}{E[y]})$\n",
    "    - (B) $\\overset{(t)}{W}{}^{-1}(y- \\overset{(t)}{E[y]})$\n",
    "    - (C) $X^T (y- \\overset{(t)}{E[y]})$\n",
    "    - (D) $X^T (\\tilde y^{(t)}-X\\beta^{(t)})$\n",
    "\n",
    "    \n",
    "5. (0.5 points) Which of the following is the most directly efficient computation of ***Newton's method***?\n",
    "\n",
    "    - (A) $\\tilde y^{(t)} = \\underline{X\\beta^{(t)}}+ \\overset{(t)}{W}{}^{-1}(y- \\overset{(t)}{E[y]})$\n",
    "    - (B) $\\beta^{(t+1)} = \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} \\tilde y^{(t)}$\n",
    "    - (C) $\\beta^{(t+1)} = \\underline{\\beta^{(t)}} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T\\overset{(t)}{W} (\\tilde y^{(t)}-\\underline{X\\beta^{(t)}})$\n",
    "    - (D) $\\beta^{(t+1)} = \\beta^{(t)} + \\left(X^T \\overset{(t)}{W}X\\right)^{-1} X^T (y- \\overset{(t)}{E[y]})$\n",
    "    \n",
    "\n",
    "6. (0.5 points) What is the fundamental technical difference between ***Newton's method*** and ***Fisher scoring***? \n",
    "\n",
    "    - (A) They produce numerically different answers\n",
    "    - (B) Potential differences between the negative ***Hessian*** and ***Fisher information***\n",
    "    - (C) ***Newton's method*** solves IRLS through iterative update while ***Fisher scoring*** solves it as a ***weighted least squares regression problem***\n",
    "    - (D) Nothing since they are equivalent as demonstrated here\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3adfdf7",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 0.5 points each [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p1q2 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q3 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q4 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q5 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p1q6 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# Uncomment the above and keep each only either \"A\" or \"B\" or \"C\" or \"D\"\n",
    "\n",
    "# This cell will produce a runtime error until the `p1q2`-`p1q6` variables are assigned values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331e2e48",
   "metadata": {},
   "source": [
    "# Problem 2 (5 points)\n",
    "\n",
    "Provide a model fit that is robust against outliers by defining the function `huber_SLR(x, y, c, K=10, eps=1e-7)` (for simple linear regression) which implements ***M estimation*** for ***Huber loss*** with the ***IRLS*** algorithm where the weight of a data point is $1$ if $|\\psi_c(y_i-x_i^t\\beta^{(t)})| \\leq c$ and $0$ otherwise, where  \n",
    "\n",
    "$$\\psi_c(t) = \\left\\{\\begin{array}{ll}c&t>c\\\\t & |t| \\leq c\\\\-c&t<-c\\end{array}\\right.$$\n",
    "\n",
    "\n",
    "***Hints:*** \n",
    "\n",
    "- Your `huber_SLR` function will be tested directly using data simiar to the example below.\n",
    "- Use the `OLS` function rather than the `WLS` function by subsetting the data to only the points with weight $1$.\n",
    "- This algorithm is specified in the course notes as well as in Keith Knight's STA410 [notes14.pdf document](https://q.utoronto.ca/courses/296804/files?preview=25407629)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1364a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 100\n",
    "np.random.seed(1)\n",
    "x = stats.norm.rvs(size=n)\n",
    "y = 5*x + (1+np.abs(x))*stats.norm().rvs(size=n) \n",
    "fig,ax = plt.subplots(1,2,figsize=(10,5))\n",
    "ax[0].plot(x,y,'.', label=\"Clean Data\")\n",
    "\n",
    "# https://www.statsmodels.org/dev/generated/statsmodels.regression.linear_model.OLS.html\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y,X)\n",
    "fit = model.fit()\n",
    "support = np.linspace(-3,3,20)\n",
    "ax[0].plot(support,fit.predict(sm.add_constant(support)),\n",
    "           label=\"Clean Data Model Fit\")\n",
    "ax[0].legend()\n",
    "\n",
    "n_corrupted = 10\n",
    "np.random.shuffle(x[:n_corrupted])\n",
    "ax[1].plot(support,fit.predict(sm.add_constant(support)),\n",
    "           label=\"Clean Data Model Fit\")\n",
    "ax[1].plot(x,y,'.', label=\"Corrupted Data\")\n",
    "X = sm.add_constant(x)\n",
    "model = sm.OLS(y,X)\n",
    "fit = model.fit()\n",
    "support = np.linspace(-3,3,20)\n",
    "ax[1].plot(support,fit.predict(sm.add_constant(support)),\n",
    "           label=\"Corrupted Data Model Fit\")\n",
    "ax[1].legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ea6ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def `huber_SLR(x, y, c, K=10, eps=1e-7)`\n",
    "    '''\n",
    "        Fits a simple linear regression y=ax+b using \n",
    "        huber loss with arbitrary tuning parameter c\n",
    "        \n",
    "        x:   (np.array)  independent variable\n",
    "        y:   (np.array)  dependent variable\n",
    "        c:   (float)     |y_i-yhat_i|>c makes w_i=0; otherwise, w_i=1\n",
    "        K:   (int)       maximum IRLS steps\n",
    "        eps: (float)     stopping criterion returns IRLS fit at step k\n",
    "                         if ||(a_k,b_k)-(a_{k-1},b_{k-1})||_2^2 < eps\n",
    "                         \n",
    "        returns IRLS fit (statsmodels OLS) object\n",
    "                after K steps of when eps stopping criterion is met\n",
    "    '''\n",
    "\n",
    "    X = sm.add_constant(x); model = sm.OLS(y,X); fit = model.fit()\n",
    "    params = fit.params\n",
    "    \n",
    "    # Complete the K-step Huber loss IRLS algorithm updating `fit`\n",
    "    # incorporating an eps-based easly stopping criterion\n",
    "    \n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59626d3e",
   "metadata": {},
   "source": [
    "## Problem 2 question 0-2 (2 points)\n",
    "\n",
    "0-2. Your `huber_SLR` will be tested on the data above for various choices of `c`, `K`, and `eps`.\n",
    "\n",
    "- You do not need to assign any variables for this problem -- your `huber_SLR` function will be called directly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05395571",
   "metadata": {},
   "source": [
    "## Problem 2 question 3 (1 point)\n",
    "\n",
    "3. Which of the following is the smallest integer value for $c$ which first makes the simple linear regression fit of the `huber_SLR` extremely similar to the \"Clean Data Model Fit\" for the data ?\n",
    "\n",
    "    1. 1\n",
    "    2. 2\n",
    "    3. 3\n",
    "    4. 4\n",
    "\n",
    "***Hint:*** if you replace `ax[1].legend()` with \n",
    "\n",
    "```python\n",
    "huber_fit = huber_SLR(x,y,c=2,K=10)\n",
    "ax[1].plot(support,huber_fit.predict(sm.add_constant(support)),\n",
    "           label=\"Huber Loss Corrupted Data Model Fit\")\n",
    "ax[1].legend()\n",
    "```\n",
    "\n",
    "in the plotting demonstration code above you can see the simple linear regression line fit with the ***Huber loss*** function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24bdab61",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 1.0 point [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" based on the choices above]\n",
    "p2q3 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "# Uncomment the above and keep each only either \"A\" or \"B\" or \"C\" or \"D\"\n",
    "\n",
    "# This cell will produce a runtime error until the `p2q3` variable is assigned a value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40f0660f",
   "metadata": {},
   "source": [
    "## Problem 2 question 4-6 (1 point)\n",
    "\n",
    "4-6. Define a new function `huber_MLR` which generalizes the simpler linear regression function `huber_SLR` to accept a multivariate design matrix `X` rather than a vector `x`. When defining `huber_MLR` follow the specifications given in the starter code below.\n",
    "\n",
    "- You do not need to assign any variables for this problem -- your `huber_MLR` function will be tested directly for some design matrix `X` and various choices of `c`, `K`, and `eps`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b8a8aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def huber_MLR(x, y, c, K=10, eps=1e-7):\n",
    "    '''\n",
    "        Fits a multivariate linear regression y = X beta using \n",
    "        huber loss with arbitrary tuning parameter c\n",
    "        \n",
    "        X(n,p): (np.array)  design matrix (intercept will not be added)\n",
    "        y(n,):  (np.array)  dependent variable\n",
    "        c:      (float)     |y_i-yhat_i|>c makes w_i=0; otherwise, w_i=1\n",
    "        K:      (int)       maximum IRLS steps (default K=10)\n",
    "        eps:    (float)     stopping criterion returns IRLS fit at step k\n",
    "                            if ||(a_k,b_k)-(a_{k-1},b_{k-1})||_2^2 < eps (default eps=1e-7)\n",
    "                         \n",
    "        returns IRLS fit (statsmodels OLS) object\n",
    "                after K steps of when eps stopping criterion is met\n",
    "    '''\n",
    "\n",
    "    # Complete the K-step Huber loss IRLS algorithm updating `fit`\n",
    "    # incorporating an eps-based easly stopping criterion\n",
    "    \n",
    "    model = sm.OLS(y,X); fit = model.fit()\n",
    "    params = fit.params\n",
    "    \n",
    "    return fit"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfc60a8",
   "metadata": {},
   "source": [
    "## Problem 2 questions 7-8 (1 point)\n",
    "\n",
    "7. What is true about the ***M estimation*** for ***Huber loss*** in problem 2 and logistic regression model fitting in problem 1?\n",
    "\n",
    "    1. They both substitue ***Fisher information*** for the expected value of the ***Hessian***\n",
    "    2. Problem 2 specifies a minimization problem while problem 1 specifies a maximization problem\n",
    "    3. Problem 1 is implemented using an ***IRLS*** algorithm while ***M estimation*** for ***Huber loss*** in problem 2 is not\n",
    "    4. All of the above\n",
    "\n",
    "\n",
    "8. For ***Huber loss*** fit with ***M estimation*** as above, which of the following is the same as $E[X^TWX]$, where \n",
    "\n",
    "   $$W_{ii}=\\left\\{\\begin{array}{ll} 1 & |y_i-x_i^T\\beta|\\leq c\\\\0&\\text{otherwise} \\end{array}\\right. \\quad \\text{ and } \\quad W_{ij}=0 \\text{ for } i\\neq j$$ \n",
    "\n",
    "   and $c$ is the ***Huber loss*** function parameter and $y_i$ is assumed to be independently and identically distributed for all $i$?\n",
    "\n",
    "    1. $\\Pr(|y_i-x_i^T\\beta|\\leq c)X^TX$ \n",
    "    2. ***expected Fisher Information***\n",
    "    3. ***observed Fisher Information***\n",
    "    4. The inverse of the negative ***Hessian*** matrix\n",
    "    5. All of the above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b33a475",
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# 0.5 points each [format: `str` either \"A\" or \"B\" or \"C\" or \"D\" or \"E\" based on the choices above]\n",
    "p2q7 = #<\"A\"|\"B\"|\"C\"|\"D\"> \n",
    "p2q8 = #<\"A\"|\"B\"|\"C\"|\"D\"|\"E\"> \n",
    "# Uncomment the above and keep each only either \"A\" or \"B\" or \"C\" or \"D\" or \"E\"\n",
    "\n",
    "# This cell will produce a runtime error until the `p2q7` and `p2q8` variables are assigned values"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
